\chapter{Autonomous Driving}

Autonomous driving is a complex task. The vehicle collects information about the surrounding environment from its sensors and then it decides its next control input to the actuators of the vehicle. There are two general approaches to this problem: end-to-end driving and decomposition into several subproblems.

The end-to-end driving approach takes the sensor data as an input and maps them to the control inputs. The end-to-end driving algorithm can be implemented for example using a stream of images from a front-facing camera and a neural network trained using supervised or reinforcement learning. It was successfully demonstrated for example in ALVINN, a simple 3-layer neutral network used in the NavLab autonomous vehicle of Carnegie Mellon University in 1989 \cite{ALVINN}, or more recently with a deep convolutional neural network by Nvidia \cite{Nvidia}. End-to-end driving avoids explicit modeling of the world and the vehicle and it relies on the knowledge extracted from the training data or by learning by trial and error during reinforcement learning.

The other approach is to split the complex problem into several smaller problems, which are solved independently. We can split the problem into three main subproblems: Perception, Planning, and Control.

Perception is the process of collecting data from the sensors and processing them to obtain the current state of the world and the internal properties of the vehicle. The task of determining the position of the vehicle on a map is called localization. The sensors which could be used for localization are cameras, radars, ultrasonic sensors, and LIDARs. The data from these sensors can be used to determine distances to nearby obstacles in different directions. Based on the previous known position of the vehicle, the estimate of its movement, and the distances to obstacles in different directions, the position on a known map can be determined using an algorithm such as Adaptive Monte Carlo Localization (AMCL) \cite{AMCL_position_estimation} \cite{AMCL_adaptive_sampling}. In certain scenarios, the relative distances to the obstacles might not be enough to determine correct position. This can happen for example when the vehicle is driving through a straight tunnel and the distances to the walls are constant even though the vehicle is moving. It is useful to combine readings from multiple sensors which provide odometry such as wheel encoders which measure how many times the wheels turn and an \gls*{IMU} with gyroscopes and accelerometers which measures the linear and angular acceleration of the vehicle. The combination of data from multiple different types of sensors is called sensor fusion. Kalman filter is an example of an algorithm which is frequently used to fuse data from different sources \cite{Kalman_filter}.

The vehicle must also be able to read road signs and road surface markings for driving along public roads. The data from the sensors can also be used to detect obstacles and for obstacle tracking. The type of obstacle is then identified and in the case of dynamic obstacles such as other cars, bicyclists, pedestrians, animals, or moving inanimate objects their movement in time must be predicted to prevent collisions.

The geometry of a car-like vehicle limits its controllability. The vehicle cannot turn while it is stopped and it can only start turning once it is moving forwards or backwards. Usually the only way to control the turning radius is by turning the front wheels and the rear wheels are fixed. In order to be able to make any reasoning about the future states of the vehicle, we must be able to predict the effect of a sequence of control inputs on the motion of the vehicle. Without an accurate model of the vehicle we could select a trajectory which cannot be safely followed, or which would be ineffective. We will discuss vehicle modelling in detail in chapter Vehicle Model.

With the knowledge of the vehicle model, we can search for a plan consisting of control inputs over some time period which will result in a collision-free trajectory through the environment which minimizes some cost function (e.g., the time it will take to reach some goal location). Planning several steps into the future can give us an advantage over a simple end-to-end driving approach. For example, in the case of autonomous racing, the agent can take advantage of the knowledge of the racing track map, and account for the shape of the track hidden around the next corner. We should be able to decide, when to slow down and when to accelerate, as well as at which point to start turning into a corner, in order to reach the best lap times. We call this subproblem trajectory planning.

Executing the sequence of control inputs one by one might cause the vehicle to drive off the track. The trajectory planning algorithm relies on a vehicle model which might not be accurate. The reference trajectory is therefore idealized, and it might not be possible to achieve it exactly by the actual vehicle. It might also lead to a collision with an obstacle on the track which was not known at the time of planning. A trajectory following algorithm chooses the next action based on the current state of the vehicle and the distance to the position, vehicle orientation, and speed at a corresponding point on the reference trajectory. The algorithm should also avoid any unexpected obstacles detected by the sensors.

The actual effects of the control inputs are measured by the sensors and the control algorithm tries to minimize the error between the planned trajectory and the actual trajectory of the vehicle in its next step. This process is referred to in control theory as closed feedback loop.

\section{Related Work}

\paragraph{DARPA Urban Challenge} In 2007, the DARPA Urban Challenge took place in the USA. The challenge was successfully completed by several vehicles with different approaches to trajectory planning. The entry from the Carnegie Mellon University, Boss, won the competition. It uses the Anytime D* algorithm for navigation in unstructured environment \cite{Boss}. The Stanford team used an algorithm called Hybrid A* in their vehicle called Junior \cite{Junior}. This algorithm is an extension of the A* algorithm which discretizes the continuous search space into a discrete grid to avoid examining similar configurations multiple times, but it keeps the information about the trajectories in the continuous configuration space to produce smooth feasible trajectories. The team from MIT used modified RRT algorithm in their vehicle \cite{RRT_urban_driving}.

\paragraph{MIT RACECAR} RACECAR (Rapid Autonomous Complex-Environment \\ Competing Ackermann-steering Robot) is a project at MIT which uses an RC car to test various perception, planning, and control algorithms. The cars are used by students in various courses at MIT\footnote{\url{https://racecar.mit.edu/education}}.

\paragraph{AutoRally} The Georgia Tech uses an RC-car based platform AutoRally to test algorithms for self-driving cars. The publications of algorithms tested on AutoRally include both classical planning-based approaches, such as MPC, and also deep learning end-to-end driving policies\footnote{\url{https://autorally.github.io/}}.

\paragraph{The F1/10 Competition}

This problem is inspired by the F1/10 competition organized by the University of Pennsylvania and the University of Virginia \cite{F1/10_web}. We will use the resources from this competition to build a similar autonomous vehicle based on an RC car. To evaluate the performance of the vehicle, we will use the criteria of Time Trial Race of F1/10. In the Time Trial Race, the vehicle drives for 5 minutes around a circuit trying to achieve as many laps as possible without a collision with the boundary of the track or with static obstacles on the track. The time of the fastest lap is also recorded.

\paragraph{CTU F1/10 Research} A team at the Czech Technical University in Prague called ``Řeřicha'' successfully entered the F1/10 competition several times. In the April of 2018, the team won the competition in Porto, Portugal. One of the students, Martin Vajnar, described the construction of the vehicle in his Master Thesis \cite{ctu_martin_vajnar} and the source code of the algorithm is publicly available on GitHub\footnote{\url{https://github.com/f1tenth/F110CPSWeek2018/tree/master/CTU}}.

The team succeeded with a purely reactive algorithm called ``Follow The Gap'' which processes the raw data from a LIDAR and for each reading, it selects a steering angle in the direction where there is no obstacle (in the direction of the biggest ``gap'') in the laser scan data and it adjusts the speed based on the steering angle (when the car drives mostly straight, the speed is high, when the car turns, the speed is lower). Later in the same year, the CTU team finished third in Torino, Italy.

Jan Dusil describes an updated hardware of the vehicle of the CTU team and he describes algorithms for detecting the slip angle of the vehicle in his Bachelor Thesis \cite{ctu_jan_dusil}. Jaroslav Klapálek describes an algorithm for dynamic obstacle avoidance for F1/10 car where he solves the problem of distributed intersection control in his Master Thesis \cite{ctu_jaroslav_klapalek}.

\paragraph{Roborace} is a competition of full-scale autonomous cars. All contestants use the same hardware, a specially designed racing vehicle called \textit{Robocar}. The first race which was held in 2019 was won by a team from Technical University of Munich. This team published several papers about the algorithms and methods they used, including the trajectory planning algorithm \cite{tum_roborace_planning}, an extension of the AMCL ROS package \cite{tum_roborace_ros}, and state estimation method \cite{tum_roborace_state_estimation}.